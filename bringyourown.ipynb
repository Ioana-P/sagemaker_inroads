{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## model training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import string \n",
    "import re \n",
    "import pandas as pd\n",
    "import boto3\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inroads = pd.read_csv('https://inroads-test-bucket1.s3.amazonaws.com/inroads_only.csv', dtype={'text': str})\n",
    "#df_raw = pd.read_csv('https://inroads-test-bucket1.s3.amazonaws.com/raw_data.csv', dtype={'text': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_clean = df_inroads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lowercase(text): \n",
    "    return text.lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text): \n",
    "    result = re.sub(r'\\d+', '', text) \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    temp = text.translate(translator)\n",
    "    return re.sub('\\s+',' ', temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "# lemmatize string \n",
    "\n",
    "def lemmatize_word(text): \n",
    "    word_tokens = word_tokenize(text) \n",
    "    # provide context i.e. part-of-speech \n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    return lemmas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_words(text):\n",
    "    clean_words = []\n",
    "    for word in text:\n",
    "        new = re.sub(r'\\b\\w{,3}\\b', '', word)\n",
    "        clean_words.append(new) \n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list()\n",
    "\n",
    "for i in range(0,len(text_to_clean)):\n",
    "    temp = str(text_to_clean.text.iloc[i])     \n",
    "    temp = text_lowercase(temp) #make all lower\n",
    "    temp_2 = remove_numbers(temp) # remove all numbers\n",
    "    temp_3 = str.strip(temp_2) # remove white spaces 1\n",
    "    temp_4 = temp_3.replace('\\n',' ').replace(\"\\t\", \" \") # remove white spaces 2\n",
    "    temp_5 = re.sub(\" +\", \" \", temp_4) # remove white spaces 3\n",
    "    temp_6 = remove_punctuation(temp_5) # remove punctuation \n",
    "    temp_7 = lemmatize_word(temp_6) # lemmatize text\n",
    "    temp_8 = [word for word in temp_7 if word.isalnum()] # remove auxiliarly punctuation\n",
    "    temp_9 = remove_short_words(temp_8) # remove short words\n",
    "    result.append(list(filter(None, temp_9))) # remove empty strings and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_clean['clean_text'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "## review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       Unnamed: 0                                           resource  \\\n",
       "0              1  649607\\nresearch-article2016\\n                ...   \n",
       "1              2  649607\\nresearch-article2016\\n                ...   \n",
       "2              3  649607\\nresearch-article2016\\n                ...   \n",
       "3              4  649607\\nresearch-article2016\\n                ...   \n",
       "4              5  649607\\nresearch-article2016\\n                ...   \n",
       "...          ...                                                ...   \n",
       "1221        1222  the-inroads-team-is-growing-introducing-our-ne...   \n",
       "1222        1223  there-is-no-qualification-to-be-here-just-pass...   \n",
       "1223        1224                                           wfac.txt   \n",
       "1224        1225  what-is-inspiring-our-steering-committee-in-20...   \n",
       "1225        1226  young-people-fight-societal-norms-in-sudan-whe...   \n",
       "\n",
       "                                                   text  \\\n",
       "0     649607\\nresearch-article2016\\n                ...   \n",
       "1     Huang et al.                                  ...   \n",
       "2     972                                           ...   \n",
       "3     Huang et al.                                  ...   \n",
       "4     974\\n      Table 1. Descriptive Statistics and...   \n",
       "...                                                 ...   \n",
       "1221  \\nSince the network launch in 2013 and the ope...   \n",
       "1222  \\nThe 2nd Bi-annual inroads Global Member Gath...   \n",
       "1223  \\nCameroon is one of seven African nations tha...   \n",
       "1224  \\nThis year brings us a group of committed ind...   \n",
       "1225  \\nWe often hear from many of our members, in d...   \n",
       "\n",
       "                                             text_lower  \\\n",
       "0     649607\\nresearch-article2016\\n                ...   \n",
       "1     huang et al.                                  ...   \n",
       "2     972                                           ...   \n",
       "3     huang et al.                                  ...   \n",
       "4     974\\n      table 1. descriptive statistics and...   \n",
       "...                                                 ...   \n",
       "1221  \\nsince the network launch in 2013 and the ope...   \n",
       "1222  \\nthe 2nd bi-annual inroads global member gath...   \n",
       "1223  \\ncameroon is one of seven african nations tha...   \n",
       "1224  \\nthis year brings us a group of committed ind...   \n",
       "1225  \\nwe often hear from many of our members, in d...   \n",
       "\n",
       "                                             clean_text  \n",
       "0     [research, article, pspxxx, personality, socia...  \n",
       "1     [huang, describe, current, program, research, ...  \n",
       "2     [personality, social, psychology, bulletin, mo...  \n",
       "3     [huang, abortion, attitudes, over, year, perio...  \n",
       "4     [table, descriptive, statistics, bivariate, co...  \n",
       "...                                                 ...  \n",
       "1221  [since, network, launch, open, membership, int...  \n",
       "1222  [annual, inroads, global, member, gather, take...  \n",
       "1223  [cameroon, seven, african, nations, that, upho...  \n",
       "1224  [this, year, bring, group, commit, individuals...  \n",
       "1225  [often, hear, from, many, members, digital, re...  \n",
       "\n",
       "[1226 rows x 5 columns]>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_clean.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [research, article, pspxxx, personality, socia...\n",
       "1     [huang, describe, current, program, research, ...\n",
       "2     [personality, social, psychology, bulletin, mo...\n",
       "3     [huang, abortion, attitudes, over, year, perio...\n",
       "4     [table, descriptive, statistics, bivariate, co...\n",
       "                            ...                        \n",
       "95    [towards, asps, show, figure, informants, expr...\n",
       "96    [behavior, community, towards, people, associa...\n",
       "97    [abortion, brother, stop, talk, with, after, l...\n",
       "98    [person, support, abortion, spaw, fgds, partic...\n",
       "99    [women, exclude, religious, practice, abortive...\n",
       "Name: clean_text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_clean.clean_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save cleaned tokens for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_inroads_clean = text_to_clean[['resource', 'clean_text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                resource  \\\n",
       "0     649607\\nresearch-article2016\\n                ...   \n",
       "1     649607\\nresearch-article2016\\n                ...   \n",
       "2     649607\\nresearch-article2016\\n                ...   \n",
       "3     649607\\nresearch-article2016\\n                ...   \n",
       "4     649607\\nresearch-article2016\\n                ...   \n",
       "...                                                 ...   \n",
       "1221  the-inroads-team-is-growing-introducing-our-ne...   \n",
       "1222  there-is-no-qualification-to-be-here-just-pass...   \n",
       "1223                                           wfac.txt   \n",
       "1224  what-is-inspiring-our-steering-committee-in-20...   \n",
       "1225  young-people-fight-societal-norms-in-sudan-whe...   \n",
       "\n",
       "                                             clean_text  \n",
       "0     [research, article, pspxxx, personality, socia...  \n",
       "1     [huang, describe, current, program, research, ...  \n",
       "2     [personality, social, psychology, bulletin, mo...  \n",
       "3     [huang, abortion, attitudes, over, year, perio...  \n",
       "4     [table, descriptive, statistics, bivariate, co...  \n",
       "...                                                 ...  \n",
       "1221  [since, network, launch, open, membership, int...  \n",
       "1222  [annual, inroads, global, member, gather, take...  \n",
       "1223  [cameroon, seven, african, nations, that, upho...  \n",
       "1224  [this, year, bring, group, commit, individuals...  \n",
       "1225  [often, hear, from, many, members, digital, re...  \n",
       "\n",
       "[1226 rows x 2 columns]>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_inroads_clean.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_inroads_clean.to_pickle('final_inroads_clean.pkl')\n",
    "final_inroads_clean = pd.read_pickle('final_inroads_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['research', 'article', 'pspxxx', 'personality', 'social', 'psychology', 'bulletinhuang', 'article', 'personality', 'social', 'benevolent', 'sexism', 'attitudes', 'toward', 'psychology', 'bulletin', 'society', 'personality', 'motherhood', 'reproductive', 'right', 'social', 'psychology', 'reprint', 'permissions', 'multi', 'study', 'longitudinal', 'examination', 'sagepub', 'journalspermissions', 'abortion', 'attitudes', 'pspb', 'sagepub', 'yanshu', 'huang', 'paul', 'davies', 'chris', 'sibley', 'danny', 'osborne', 'abstract', 'although', 'benevolent', 'sexism', 'ideology', 'that', 'highly', 'revere', 'women', 'conform', 'traditional', 'gender', 'cloak', 'superficially', 'positive', 'tone', 'place', 'upon', 'pedestal', 'inherently', 'restrictive', 'accordingly', 'because', 'paternalistic', 'beliefs', 'associate', 'with', 'base', 'idealization', 'traditional', 'gender', 'roles', 'which', 'include', 'motherhood', 'should', 'predict', 'people', 'attitudes', 'toward', 'women', 'reproductive', 'right', 'data', 'from', 'nationwide', 'longitudinal', 'panel', 'study', 'study', 'show', 'that', 'hostile', 'sexism', 'have', 'cross', 'effect', 'opposition', 'both', 'elective', 'traumatic', 'abortion', 'study', 'extend', 'these', 'find', 'show', 'that', 'relationship', 'between', 'support', 'abortion', 'fully', 'mediate', 'attitudes', 'toward', 'motherhood', 'these', 'result', 'highlight', 'pernicious', 'nature', 'demonstrate', 'that', 'idealization', 'motherhood', 'substantial', 'cost', 'namely', 'restriction', 'women', 'reproductive', 'right', 'keywords', 'benevolent', 'sexism', 'motherhood', 'traumatic', 'abortion', 'elective', 'abortion', 'longitudinal', 'receive', 'revision', 'accept', 'april', 'motherhood', 'often', 'regard', 'essential', 'component', 'traumatic', 'woman', 'health', 'life', 'woman', 'holton', 'fisher', 'rowe', 'would', 'compromise', 'bring', 'pregnancy', 'term', 'indeed', 'become', 'mother', 'many', 'woman', 'pregnancy', 'result', 'rape', 'notably', 'support', 'highest', 'gender', 'role', 'that', 'complete', 'women', 'traumatic', 'abortion', 'higher', 'than', 'support', 'abortion', 'that', 'chrisler', 'gorman', 'marván', 'johnston', 'robledo', 'view', 'elective', 'bahr', 'marcos', 'craig', 'accordingly', 'women', 'decide', 'least', 'temporarily', 'reject', 'this', 'sacred', 'role', 'choose', 'terminate', 'pregnancy', 'current', 'study', 'contribute', 'this', 'debate', 'directly', 'often', 'meet', 'with', 'criticism', 'osborne', 'davies', 'examine', 'impact', 'people', 'gender', 'role', 'attitudes', 'have', 'observations', 'such', 'these', 'lead', 'cook', 'jelen', 'wilcox', 'their', 'support', 'both', 'elective', 'traumatic', 'abortion', 'within', 'contend', 'that', 'abortion', 'debate', 'general', 'longitudinal', 'panel', 'study', 'follow', 'cross', 'sectional', 'public', 'partially', 'about', 'appropriate', 'roles', 'women', 'study', 'before', 'describe', 'study', 'examine', 'stan', 'society', 'short', 'traditional', 'gender', 'roles', 'function', 'dard', 'socio', 'demographic', 'correlate', 'abortion', 'attitudes', 'restrict', 'women', 'reproductive', 'choices', 'then', 'introduce', 'literature', 'ambivalent', 'sexism', 'theory', 'when', 'discuss', 'debate', 'over', 'woman', 'right', 'glick', 'fiske', 'reconcile', 'some', 'inconsisten', 'choose', 'easy', 'rely', 'common', 'nomenclature', 'cies', 'identify', 'past', 'research', 'then', 'conclude', 'this', 'section', 'life', 'choice', 'imply', 'that', 'abortion', 'attitudes', 'relatively', 'uniform', 'study', 'demonstrate', 'university', 'auckland', 'zealand', 'ever', 'that', 'support', 'abortion', 'vary', 'circumstances', 'university', 'british', 'columbia', 'okanagan', 'kelowna', 'canada', 'surround', 'woman', 'decision', 'terminate', 'pregnancy', 'correspond', 'author', 'alvarez', 'brehm', 'craig', 'kane', 'martinez', 'yanshu', 'huang', 'school', 'psychology', 'university', 'auckland', 'private', 'such', 'circumstances', 'include', 'those', 'that', 'elective', 'auckland', 'zealand', 'financial', 'insecurity', 'woman', 'want', 'child', 'email', 'yhua', 'aucklanduni']]\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list \n",
    "temp_list =[] \n",
    "\n",
    "# Iterate over each row \n",
    "for index, rows in final_inroads_clean.iterrows(): \n",
    "    # Create list for the current row \n",
    "    my_list =[rows.clean_text] \n",
    "\n",
    "    # append the list to the final list \n",
    "    temp_list.append(my_list) \n",
    "\n",
    "# Print the list \n",
    "print(temp_list[0]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1226"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_inroads_tokens = []\n",
    "\n",
    "for i in range(0,len(final_inroads_clean)):\n",
    "    text = final_inroads_clean.clean_text[i]\n",
    "    #print(text)\n",
    "    clean_inroads_tokens.extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of              tokens\n",
       "0          research\n",
       "1           article\n",
       "2            pspxxx\n",
       "3       personality\n",
       "4            social\n",
       "...             ...\n",
       "321033       slowly\n",
       "321034        there\n",
       "321035        space\n",
       "321036        start\n",
       "321037   discussion\n",
       "\n",
       "[321038 rows x 1 columns]>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_inroads_tokens = pd.DataFrame(clean_inroads_tokens,columns=['tokens'])\n",
    "clean_inroads_tokens.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_inroads_tokens.to_pickle('clean_inroads_tokens.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_inroads_tokens = pd.read_pickle('clean_inroads_tokens.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "## upload trainign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "BucketAlreadyOwnedByYou",
     "evalue": "An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBucketAlreadyOwnedByYou\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-4f93697291c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m s3_session.create_bucket(Bucket=bucket, \n\u001b[1;32m      6\u001b[0m                          \u001b[0mCreateBucketConfiguration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                          {'LocationConstraint': region})\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0ms3_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train/clean_full_tokens.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'clean_full_tokens.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/resources/factory.py\u001b[0m in \u001b[0;36mdo_action\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0;31m# instance via ``self``.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mdo_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'load'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/resources/action.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, parent, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m                     operation_name, params)\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Response: %r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBucketAlreadyOwnedByYou\u001b[0m: An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "bucket = 'sagemaker-word2vec-scikitlearn'\n",
    "region = 'us-east-2'\n",
    "s3_session = boto3.Session().resource('s3')\n",
    "s3_session.create_bucket(Bucket=bucket, \n",
    "                         CreateBucketConfiguration=\n",
    "                         {'LocationConstraint': region})\n",
    "s3_session.Bucket(bucket).Object('train/clean_full_tokens.pkl').upload_file('clean_full_tokens.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = clean_inroads_tokens.to_dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': {0: 'research',\n",
       "  1: 'article',\n",
       "  2: 'pspxxx',\n",
       "  3: 'personality',\n",
       "  4: 'social',\n",
       "  5: 'psychology',\n",
       "  6: 'bulletinhuang',\n",
       "  7: 'article',\n",
       "  8: 'personality',\n",
       "  9: 'social',\n",
       "  10: 'benevolent',\n",
       "  11: 'sexism',\n",
       "  12: 'attitudes',\n",
       "  13: 'toward',\n",
       "  14: 'psychology',\n",
       "  15: 'bulletin',\n",
       "  16: 'society',\n",
       "  17: 'personality',\n",
       "  18: 'motherhood',\n",
       "  19: 'reproductive',\n",
       "  20: 'right',\n",
       "  21: 'social',\n",
       "  22: 'psychology',\n",
       "  23: 'reprint',\n",
       "  24: 'permissions',\n",
       "  25: 'multi',\n",
       "  26: 'study',\n",
       "  27: 'longitudinal',\n",
       "  28: 'examination',\n",
       "  29: 'sagepub',\n",
       "  30: 'journalspermissions',\n",
       "  31: 'abortion',\n",
       "  32: 'attitudes',\n",
       "  33: 'pspb',\n",
       "  34: 'sagepub',\n",
       "  35: 'yanshu',\n",
       "  36: 'huang',\n",
       "  37: 'paul',\n",
       "  38: 'davies',\n",
       "  39: 'chris',\n",
       "  40: 'sibley',\n",
       "  41: 'danny',\n",
       "  42: 'osborne',\n",
       "  43: 'abstract',\n",
       "  44: 'although',\n",
       "  45: 'benevolent',\n",
       "  46: 'sexism',\n",
       "  47: 'ideology',\n",
       "  48: 'that',\n",
       "  49: 'highly',\n",
       "  50: 'revere',\n",
       "  51: 'women',\n",
       "  52: 'conform',\n",
       "  53: 'traditional',\n",
       "  54: 'gender',\n",
       "  55: 'cloak',\n",
       "  56: 'superficially',\n",
       "  57: 'positive',\n",
       "  58: 'tone',\n",
       "  59: 'place',\n",
       "  60: 'upon',\n",
       "  61: 'pedestal',\n",
       "  62: 'inherently',\n",
       "  63: 'restrictive',\n",
       "  64: 'accordingly',\n",
       "  65: 'because',\n",
       "  66: 'paternalistic',\n",
       "  67: 'beliefs',\n",
       "  68: 'associate',\n",
       "  69: 'with',\n",
       "  70: 'base',\n",
       "  71: 'idealization',\n",
       "  72: 'traditional',\n",
       "  73: 'gender',\n",
       "  74: 'roles',\n",
       "  75: 'which',\n",
       "  76: 'include',\n",
       "  77: 'motherhood',\n",
       "  78: 'should',\n",
       "  79: 'predict',\n",
       "  80: 'people',\n",
       "  81: 'attitudes',\n",
       "  82: 'toward',\n",
       "  83: 'women',\n",
       "  84: 'reproductive',\n",
       "  85: 'right',\n",
       "  86: 'data',\n",
       "  87: 'from',\n",
       "  88: 'nationwide',\n",
       "  89: 'longitudinal',\n",
       "  90: 'panel',\n",
       "  91: 'study',\n",
       "  92: 'study',\n",
       "  93: 'show',\n",
       "  94: 'that',\n",
       "  95: 'hostile',\n",
       "  96: 'sexism',\n",
       "  97: 'have',\n",
       "  98: 'cross',\n",
       "  99: 'effect',\n",
       "  100: 'opposition',\n",
       "  101: 'both',\n",
       "  102: 'elective',\n",
       "  103: 'traumatic',\n",
       "  104: 'abortion',\n",
       "  105: 'study',\n",
       "  106: 'extend',\n",
       "  107: 'these',\n",
       "  108: 'find',\n",
       "  109: 'show',\n",
       "  110: 'that',\n",
       "  111: 'relationship',\n",
       "  112: 'between',\n",
       "  113: 'support',\n",
       "  114: 'abortion',\n",
       "  115: 'fully',\n",
       "  116: 'mediate',\n",
       "  117: 'attitudes',\n",
       "  118: 'toward',\n",
       "  119: 'motherhood',\n",
       "  120: 'these',\n",
       "  121: 'result',\n",
       "  122: 'highlight',\n",
       "  123: 'pernicious',\n",
       "  124: 'nature',\n",
       "  125: 'demonstrate',\n",
       "  126: 'that',\n",
       "  127: 'idealization',\n",
       "  128: 'motherhood',\n",
       "  129: 'substantial',\n",
       "  130: 'cost',\n",
       "  131: 'namely',\n",
       "  132: 'restriction',\n",
       "  133: 'women',\n",
       "  134: 'reproductive',\n",
       "  135: 'right',\n",
       "  136: 'keywords',\n",
       "  137: 'benevolent',\n",
       "  138: 'sexism',\n",
       "  139: 'motherhood',\n",
       "  140: 'traumatic',\n",
       "  141: 'abortion',\n",
       "  142: 'elective',\n",
       "  143: 'abortion',\n",
       "  144: 'longitudinal',\n",
       "  145: 'receive',\n",
       "  146: 'revision',\n",
       "  147: 'accept',\n",
       "  148: 'april',\n",
       "  149: 'motherhood',\n",
       "  150: 'often',\n",
       "  151: 'regard',\n",
       "  152: 'essential',\n",
       "  153: 'component',\n",
       "  154: 'traumatic',\n",
       "  155: 'woman',\n",
       "  156: 'health',\n",
       "  157: 'life',\n",
       "  158: 'woman',\n",
       "  159: 'holton',\n",
       "  160: 'fisher',\n",
       "  161: 'rowe',\n",
       "  162: 'would',\n",
       "  163: 'compromise',\n",
       "  164: 'bring',\n",
       "  165: 'pregnancy',\n",
       "  166: 'term',\n",
       "  167: 'indeed',\n",
       "  168: 'become',\n",
       "  169: 'mother',\n",
       "  170: 'many',\n",
       "  171: 'woman',\n",
       "  172: 'pregnancy',\n",
       "  173: 'result',\n",
       "  174: 'rape',\n",
       "  175: 'notably',\n",
       "  176: 'support',\n",
       "  177: 'highest',\n",
       "  178: 'gender',\n",
       "  179: 'role',\n",
       "  180: 'that',\n",
       "  181: 'complete',\n",
       "  182: 'women',\n",
       "  183: 'traumatic',\n",
       "  184: 'abortion',\n",
       "  185: 'higher',\n",
       "  186: 'than',\n",
       "  187: 'support',\n",
       "  188: 'abortion',\n",
       "  189: 'that',\n",
       "  190: 'chrisler',\n",
       "  191: 'gorman',\n",
       "  192: 'marván',\n",
       "  193: 'johnston',\n",
       "  194: 'robledo',\n",
       "  195: 'view',\n",
       "  196: 'elective',\n",
       "  197: 'bahr',\n",
       "  198: 'marcos',\n",
       "  199: 'craig',\n",
       "  200: 'accordingly',\n",
       "  201: 'women',\n",
       "  202: 'decide',\n",
       "  203: 'least',\n",
       "  204: 'temporarily',\n",
       "  205: 'reject',\n",
       "  206: 'this',\n",
       "  207: 'sacred',\n",
       "  208: 'role',\n",
       "  209: 'choose',\n",
       "  210: 'terminate',\n",
       "  211: 'pregnancy',\n",
       "  212: 'current',\n",
       "  213: 'study',\n",
       "  214: 'contribute',\n",
       "  215: 'this',\n",
       "  216: 'debate',\n",
       "  217: 'directly',\n",
       "  218: 'often',\n",
       "  219: 'meet',\n",
       "  220: 'with',\n",
       "  221: 'criticism',\n",
       "  222: 'osborne',\n",
       "  223: 'davies',\n",
       "  224: 'examine',\n",
       "  225: 'impact',\n",
       "  226: 'people',\n",
       "  227: 'gender',\n",
       "  228: 'role',\n",
       "  229: 'attitudes',\n",
       "  230: 'have',\n",
       "  231: 'observations',\n",
       "  232: 'such',\n",
       "  233: 'these',\n",
       "  234: 'lead',\n",
       "  235: 'cook',\n",
       "  236: 'jelen',\n",
       "  237: 'wilcox',\n",
       "  238: 'their',\n",
       "  239: 'support',\n",
       "  240: 'both',\n",
       "  241: 'elective',\n",
       "  242: 'traumatic',\n",
       "  243: 'abortion',\n",
       "  244: 'within',\n",
       "  245: 'contend',\n",
       "  246: 'that',\n",
       "  247: 'abortion',\n",
       "  248: 'debate',\n",
       "  249: 'general',\n",
       "  250: 'longitudinal',\n",
       "  251: 'panel',\n",
       "  252: 'study',\n",
       "  253: 'follow',\n",
       "  254: 'cross',\n",
       "  255: 'sectional',\n",
       "  256: 'public',\n",
       "  257: 'partially',\n",
       "  258: 'about',\n",
       "  259: 'appropriate',\n",
       "  260: 'roles',\n",
       "  261: 'women',\n",
       "  262: 'study',\n",
       "  263: 'before',\n",
       "  264: 'describe',\n",
       "  265: 'study',\n",
       "  266: 'examine',\n",
       "  267: 'stan',\n",
       "  268: 'society',\n",
       "  269: 'short',\n",
       "  270: 'traditional',\n",
       "  271: 'gender',\n",
       "  272: 'roles',\n",
       "  273: 'function',\n",
       "  274: 'dard',\n",
       "  275: 'socio',\n",
       "  276: 'demographic',\n",
       "  277: 'correlate',\n",
       "  278: 'abortion',\n",
       "  279: 'attitudes',\n",
       "  280: 'restrict',\n",
       "  281: 'women',\n",
       "  282: 'reproductive',\n",
       "  283: 'choices',\n",
       "  284: 'then',\n",
       "  285: 'introduce',\n",
       "  286: 'literature',\n",
       "  287: 'ambivalent',\n",
       "  288: 'sexism',\n",
       "  289: 'theory',\n",
       "  290: 'when',\n",
       "  291: 'discuss',\n",
       "  292: 'debate',\n",
       "  293: 'over',\n",
       "  294: 'woman',\n",
       "  295: 'right',\n",
       "  296: 'glick',\n",
       "  297: 'fiske',\n",
       "  298: 'reconcile',\n",
       "  299: 'some',\n",
       "  300: 'inconsisten',\n",
       "  301: 'choose',\n",
       "  302: 'easy',\n",
       "  303: 'rely',\n",
       "  304: 'common',\n",
       "  305: 'nomenclature',\n",
       "  306: 'cies',\n",
       "  307: 'identify',\n",
       "  308: 'past',\n",
       "  309: 'research',\n",
       "  310: 'then',\n",
       "  311: 'conclude',\n",
       "  312: 'this',\n",
       "  313: 'section',\n",
       "  314: 'life',\n",
       "  315: 'choice',\n",
       "  316: 'imply',\n",
       "  317: 'that',\n",
       "  318: 'abortion',\n",
       "  319: 'attitudes',\n",
       "  320: 'relatively',\n",
       "  321: 'uniform',\n",
       "  322: 'study',\n",
       "  323: 'demonstrate',\n",
       "  324: 'university',\n",
       "  325: 'auckland',\n",
       "  326: 'zealand',\n",
       "  327: 'ever',\n",
       "  328: 'that',\n",
       "  329: 'support',\n",
       "  330: 'abortion',\n",
       "  331: 'vary',\n",
       "  332: 'circumstances',\n",
       "  333: 'university',\n",
       "  334: 'british',\n",
       "  335: 'columbia',\n",
       "  336: 'okanagan',\n",
       "  337: 'kelowna',\n",
       "  338: 'canada',\n",
       "  339: 'surround',\n",
       "  340: 'woman',\n",
       "  341: 'decision',\n",
       "  342: 'terminate',\n",
       "  343: 'pregnancy',\n",
       "  344: 'correspond',\n",
       "  345: 'author',\n",
       "  346: 'alvarez',\n",
       "  347: 'brehm',\n",
       "  348: 'craig',\n",
       "  349: 'kane',\n",
       "  350: 'martinez',\n",
       "  351: 'yanshu',\n",
       "  352: 'huang',\n",
       "  353: 'school',\n",
       "  354: 'psychology',\n",
       "  355: 'university',\n",
       "  356: 'auckland',\n",
       "  357: 'private',\n",
       "  358: 'such',\n",
       "  359: 'circumstances',\n",
       "  360: 'include',\n",
       "  361: 'those',\n",
       "  362: 'that',\n",
       "  363: 'elective',\n",
       "  364: 'auckland',\n",
       "  365: 'zealand',\n",
       "  366: 'financial',\n",
       "  367: 'insecurity',\n",
       "  368: 'woman',\n",
       "  369: 'want',\n",
       "  370: 'child',\n",
       "  371: 'email',\n",
       "  372: 'yhua',\n",
       "  373: 'aucklanduni',\n",
       "  374: 'huang',\n",
       "  375: 'describe',\n",
       "  376: 'current',\n",
       "  377: 'program',\n",
       "  378: 'research',\n",
       "  379: 'associate',\n",
       "  380: 'ambivalent',\n",
       "  381: 'sexism',\n",
       "  382: 'theory',\n",
       "  383: 'hypotheses',\n",
       "  384: 'help',\n",
       "  385: 'establish',\n",
       "  386: 'direction',\n",
       "  387: 'maintenance',\n",
       "  388: 'traditional',\n",
       "  389: 'gender',\n",
       "  390: 'relationship',\n",
       "  391: 'between',\n",
       "  392: 'ambivalent',\n",
       "  393: 'sexism',\n",
       "  394: 'abortion',\n",
       "  395: 'roles',\n",
       "  396: 'attitudes',\n",
       "  397: 'study',\n",
       "  398: 'identify',\n",
       "  399: 'process',\n",
       "  400: 'through',\n",
       "  401: 'which',\n",
       "  402: 'these',\n",
       "  403: 'relationships',\n",
       "  404: 'occur',\n",
       "  405: 'study',\n",
       "  406: 'these',\n",
       "  407: 'although',\n",
       "  408: 'gender',\n",
       "  409: 'role',\n",
       "  410: 'attitudes',\n",
       "  411: 'critical',\n",
       "  412: 'predictors',\n",
       "  413: 'turn',\n",
       "  414: 'review',\n",
       "  415: 'literature',\n",
       "  416: 'correlate',\n",
       "  417: 'attitudes',\n",
       "  418: 'position',\n",
       "  419: 'abortion',\n",
       "  420: 'debate',\n",
       "  421: 'ambivalent',\n",
       "  422: 'sexism',\n",
       "  423: 'theory',\n",
       "  424: 'toward',\n",
       "  425: 'abortion',\n",
       "  426: 'glick',\n",
       "  427: 'fiske',\n",
       "  428: 'argue',\n",
       "  429: 'that',\n",
       "  430: 'women',\n",
       "  431: 'subordination',\n",
       "  432: 'maintain',\n",
       "  433: 'complementary',\n",
       "  434: 'ideologies',\n",
       "  435: 'hostile',\n",
       "  436: 'sexism',\n",
       "  437: 'benevolent',\n",
       "  438: 'sexism',\n",
       "  439: 'whereas',\n",
       "  440: 'correlate',\n",
       "  441: 'abortion',\n",
       "  442: 'attitudes',\n",
       "  443: 'punish',\n",
       "  444: 'norm',\n",
       "  445: 'violate',\n",
       "  446: 'women',\n",
       "  447: 'root',\n",
       "  448: 'idealize',\n",
       "  449: 'numerous',\n",
       "  450: 'demographic',\n",
       "  451: 'attitudinal',\n",
       "  452: 'variables',\n",
       "  453: 'have',\n",
       "  454: 'view',\n",
       "  455: 'women',\n",
       "  456: 'conform',\n",
       "  457: 'traditional',\n",
       "  458: 'gender',\n",
       "  459: 'roles',\n",
       "  460: 'associate',\n",
       "  461: 'with',\n",
       "  462: 'attitudes',\n",
       "  463: 'toward',\n",
       "  464: 'abortion',\n",
       "  465: 'perhaps',\n",
       "  466: 'unsurpris',\n",
       "  467: 'thus',\n",
       "  468: 'constrain',\n",
       "  469: 'women',\n",
       "  470: 'right',\n",
       "  471: 'praise',\n",
       "  472: 'those',\n",
       "  473: 'ingly',\n",
       "  474: 'research',\n",
       "  475: 'indicate',\n",
       "  476: 'that',\n",
       "  477: 'church',\n",
       "  478: 'attendance',\n",
       "  479: 'ellison',\n",
       "  480: 'adhere',\n",
       "  481: 'traditional',\n",
       "  482: 'gender',\n",
       "  483: 'roles',\n",
       "  484: 'while',\n",
       "  485: 'devalue',\n",
       "  486: 'those',\n",
       "  487: 'echevarria',\n",
       "  488: 'smith',\n",
       "  489: 'lynxwiler',\n",
       "  490: 'jelen',\n",
       "  491: 'violate',\n",
       "  492: 'these',\n",
       "  493: 'idealize',\n",
       "  494: 'prescriptions',\n",
       "  495: 'glick',\n",
       "  496: 'diebold',\n",
       "  497: 'wilcox',\n",
       "  498: 'religiosity',\n",
       "  499: 'general',\n",
       "  500: 'adebayo',\n",
       "  501: 'bailey',\n",
       "  502: 'werner',\n",
       "  503: 'nevertheless',\n",
       "  504: 'because',\n",
       "  505: 'benin',\n",
       "  506: 'esposito',\n",
       "  507: 'basow',\n",
       "  508: 'strickler',\n",
       "  509: 'danigelis',\n",
       "  510: 'cloak',\n",
       "  511: 'superficially',\n",
       "  512: 'positive',\n",
       "  513: 'tone',\n",
       "  514: 'often',\n",
       "  515: 'escape',\n",
       "  516: 'negatively',\n",
       "  517: 'associate',\n",
       "  518: 'with',\n",
       "  519: 'abortion',\n",
       "  520: 'support',\n",
       "  521: 'label',\n",
       "  522: 'sexism',\n",
       "  523: 'likewise',\n",
       "  524: 'political',\n",
       "  525: 'view',\n",
       "  526: 'consistently',\n",
       "  527: 'emerge',\n",
       "  528: 'predic',\n",
       "  529: 'covert',\n",
       "  530: 'form',\n",
       "  531: 'facilitate',\n",
       "  532: 'maintenance',\n",
       "  533: 'tors',\n",
       "  534: 'abortion',\n",
       "  535: 'attitudes',\n",
       "  536: 'specifically',\n",
       "  537: 'conservatism',\n",
       "  538: 'both',\n",
       "  539: 'inequalities',\n",
       "  540: 'oftentimes',\n",
       "  541: 'with',\n",
       "  542: 'little',\n",
       "  543: 'opposition',\n",
       "  544: 'from',\n",
       "  545: 'term',\n",
       "  546: 'ideology',\n",
       "  547: 'party',\n",
       "  548: 'identification',\n",
       "  549: 'negatively',\n",
       "  550: 'asso',\n",
       "  551: 'oppress',\n",
       "  552: 'also',\n",
       "  553: 'jackman',\n",
       "  554: 'study',\n",
       "  555: 'where',\n",
       "  556: 'partici',\n",
       "  557: 'ciated',\n",
       "  558: 'with',\n",
       "  559: 'abortion',\n",
       "  560: 'support',\n",
       "  561: 'hess',\n",
       "  562: 'rueb',\n",
       "  563: 'sahar',\n",
       "  564: 'pant',\n",
       "  565: 'evaluate',\n",
       "  566: 'passages',\n",
       "  567: 'contain',\n",
       "  568: 'either',\n",
       "  569: 'atti',\n",
       "  570: 'karasawa',\n",
       "  571: 'zucker',\n",
       "  572: 'indeed',\n",
       "  573: 'negative',\n",
       "  574: 'rela',\n",
       "  575: 'tudes',\n",
       "  576: 'barreto',\n",
       "  577: 'ellemers',\n",
       "  578: 'show',\n",
       "  579: 'that',\n",
       "  580: 'tionship',\n",
       "  581: 'between',\n",
       "  582: 'political',\n",
       "  583: 'conservatism',\n",
       "  584: 'support',\n",
       "  585: 'more',\n",
       "  586: 'prejudice',\n",
       "  587: 'than',\n",
       "  588: 'women',\n",
       "  589: 'also',\n",
       "  590: 'felt',\n",
       "  591: 'greater',\n",
       "  592: 'anger',\n",
       "  593: 'woman',\n",
       "  594: 'right',\n",
       "  595: 'choose',\n",
       "  596: 'find',\n",
       "  597: 'across',\n",
       "  598: 'culture',\n",
       "  599: 'agostino',\n",
       "  600: 'toward',\n",
       "  601: 'passage',\n",
       "  602: 'less',\n",
       "  603: 'anger',\n",
       "  604: 'toward',\n",
       "  605: 'wahlberg',\n",
       "  606: 'sage',\n",
       "  607: 'than',\n",
       "  608: 'another',\n",
       "  609: 'study',\n",
       "  610: 'find',\n",
       "  611: 'that',\n",
       "  612: 'women',\n",
       "  613: 'despite',\n",
       "  614: 'consistencies',\n",
       "  615: 'find',\n",
       "  616: 'literature',\n",
       "  617: 'term',\n",
       "  618: 'encounter',\n",
       "  619: 'negative',\n",
       "  620: 'hostile',\n",
       "  621: 'attitudes',\n",
       "  622: 'from',\n",
       "  623: 'increase',\n",
       "  624: 'religious',\n",
       "  625: 'political',\n",
       "  626: 'correlate',\n",
       "  627: 'abortion',\n",
       "  628: 'attitudes',\n",
       "  629: 'their',\n",
       "  630: 'endorsement',\n",
       "  631: 'fischer',\n",
       "  632: 'study',\n",
       "  633: 'examine',\n",
       "  634: 'gender',\n",
       "  635: 'differences',\n",
       "  636: 'abortion',\n",
       "  637: 'debate',\n",
       "  638: 'together',\n",
       "  639: 'these',\n",
       "  640: 'study',\n",
       "  641: 'demonstrate',\n",
       "  642: 'that',\n",
       "  643: 'many',\n",
       "  644: 'have',\n",
       "  645: 'yield',\n",
       "  646: 'inconsistent',\n",
       "  647: 'result',\n",
       "  648: 'whereas',\n",
       "  649: 'some',\n",
       "  650: 'find',\n",
       "  651: 'that',\n",
       "  652: 'sometimes',\n",
       "  653: 'system',\n",
       "  654: 'women',\n",
       "  655: 'more',\n",
       "  656: 'supportive',\n",
       "  657: 'than',\n",
       "  658: 'woman',\n",
       "  659: 'right',\n",
       "  660: 'because',\n",
       "  661: 'women',\n",
       "  662: 'often',\n",
       "  663: 'evaluate',\n",
       "  664: 'through',\n",
       "  665: 'polarize',\n",
       "  666: 'choose',\n",
       "  667: 'patel',\n",
       "  668: 'johns',\n",
       "  669: 'patel',\n",
       "  670: 'kooverjee',\n",
       "  671: 'view',\n",
       "  672: 'derogation',\n",
       "  673: 'admiration',\n",
       "  674: 'establish',\n",
       "  675: 'show',\n",
       "  676: 'that',\n",
       "  677: 'gender',\n",
       "  678: 'unassociated',\n",
       "  679: 'with',\n",
       "  680: 'abortion',\n",
       "  681: 'attitudes',\n",
       "  682: 'respectively',\n",
       "  683: 'glick',\n",
       "  684: 'women',\n",
       "  685: 'level',\n",
       "  686: 'benin',\n",
       "  687: 'ebaugh',\n",
       "  688: 'haney',\n",
       "  689: 'esposito',\n",
       "  690: 'basow',\n",
       "  691: 'ambivalent',\n",
       "  692: 'sexism',\n",
       "  693: 'often',\n",
       "  694: 'associate',\n",
       "  695: 'with',\n",
       "  696: 'their',\n",
       "  697: 'tendency',\n",
       "  698: 'finlay',\n",
       "  699: 'note',\n",
       "  700: 'however',\n",
       "  701: 'that',\n",
       "  702: 'though',\n",
       "  703: 'gender',\n",
       "  704: 'subtype',\n",
       "  705: 'women',\n",
       "  706: 'specifically',\n",
       "  707: 'becker',\n",
       "  708: 'demonstrate',\n",
       "  709: 'tend',\n",
       "  710: 'unassociated',\n",
       "  711: 'with',\n",
       "  712: 'support',\n",
       "  713: 'abortion',\n",
       "  714: 'most',\n",
       "  715: 'that',\n",
       "  716: 'women',\n",
       "  717: 'more',\n",
       "  718: 'likely',\n",
       "  719: 'endorse',\n",
       "  720: 'beliefs',\n",
       "  721: 'case',\n",
       "  722: 'women',\n",
       "  723: 'more',\n",
       "  724: 'supportive',\n",
       "  725: 'than',\n",
       "  726: 'abortion',\n",
       "  727: 'response',\n",
       "  728: 'subtypes',\n",
       "  729: 'women',\n",
       "  730: 'hold',\n",
       "  731: 'position',\n",
       "  732: 'that',\n",
       "  733: 'when',\n",
       "  734: 'termination',\n",
       "  735: 'pregnancy',\n",
       "  736: 'seek',\n",
       "  737: 'specific',\n",
       "  738: 'incongruent',\n",
       "  739: 'with',\n",
       "  740: 'traditional',\n",
       "  741: 'gender',\n",
       "  742: 'roles',\n",
       "  743: 'more',\n",
       "  744: 'reason',\n",
       "  745: 'elective',\n",
       "  746: 'abortion',\n",
       "  747: 'thus',\n",
       "  748: 'relationship',\n",
       "  749: 'between',\n",
       "  750: 'likely',\n",
       "  751: 'endorse',\n",
       "  752: 'reaction',\n",
       "  753: 'subtypes',\n",
       "  754: 'women',\n",
       "  755: 'gender',\n",
       "  756: 'abortion',\n",
       "  757: 'attitudes',\n",
       "  758: 'more',\n",
       "  759: 'complex',\n",
       "  760: 'than',\n",
       "  761: 'appear',\n",
       "  762: 'conform',\n",
       "  763: 'traditional',\n",
       "  764: 'roles',\n",
       "  765: 'these',\n",
       "  766: 'find',\n",
       "  767: 'corroborate',\n",
       "  768: 'first',\n",
       "  769: 'blush',\n",
       "  770: 'sibley',\n",
       "  771: 'wilson',\n",
       "  772: 'research',\n",
       "  773: 'show',\n",
       "  774: 'that',\n",
       "  775: 'these',\n",
       "  776: 'gender',\n",
       "  777: 'role',\n",
       "  778: 'attitudes',\n",
       "  779: 'predict',\n",
       "  780: 'expose',\n",
       "  781: 'women',\n",
       "  782: 'violate',\n",
       "  783: 'traditional',\n",
       "  784: 'gender',\n",
       "  785: 'roles',\n",
       "  786: 'position',\n",
       "  787: 'abortion',\n",
       "  788: 'debate',\n",
       "  789: 'better',\n",
       "  790: 'than',\n",
       "  791: 'gender',\n",
       "  792: 'itself',\n",
       "  793: 'express',\n",
       "  794: 'higher',\n",
       "  795: 'level',\n",
       "  796: 'lower',\n",
       "  797: 'level',\n",
       "  798: 'than',\n",
       "  799: 'accordingly',\n",
       "  800: 'wall',\n",
       "  801: 'colleagues',\n",
       "  802: 'find',\n",
       "  803: 'that',\n",
       "  804: 'endorse',\n",
       "  805: 'expose',\n",
       "  806: 'women',\n",
       "  807: 'conform',\n",
       "  808: 'traditional',\n",
       "  809: 'gender',\n",
       "  810: 'roles',\n",
       "  811: 'ment',\n",
       "  812: 'traditional',\n",
       "  813: 'gender',\n",
       "  814: 'roles',\n",
       "  815: 'associate',\n",
       "  816: 'with',\n",
       "  817: 'opposi',\n",
       "  818: 'other',\n",
       "  819: 'word',\n",
       "  820: 'ambivalent',\n",
       "  821: 'sexism',\n",
       "  822: 'encourage',\n",
       "  823: 'dichoto',\n",
       "  824: 'tion',\n",
       "  825: 'abortion',\n",
       "  826: 'likewise',\n",
       "  827: 'benin',\n",
       "  828: 'show',\n",
       "  829: 'that',\n",
       "  830: 'mization',\n",
       "  831: 'women',\n",
       "  832: 'into',\n",
       "  833: 'saint',\n",
       "  834: 'sinners',\n",
       "  835: 'traditional',\n",
       "  836: 'gender',\n",
       "  837: 'role',\n",
       "  838: 'attitudes',\n",
       "  839: 'negatively',\n",
       "  840: 'correlate',\n",
       "  841: 'with',\n",
       "  842: 'support',\n",
       "  843: 'elective',\n",
       "  844: 'abortion',\n",
       "  845: 'whereas',\n",
       "  846: 'krishnan',\n",
       "  847: 'sanctity',\n",
       "  848: 'motherhood',\n",
       "  849: 'find',\n",
       "  850: 'that',\n",
       "  851: 'traditional',\n",
       "  852: 'gender',\n",
       "  853: 'role',\n",
       "  854: 'attitudes',\n",
       "  855: 'predict',\n",
       "  856: 'opposi',\n",
       "  857: 'tion',\n",
       "  858: 'both',\n",
       "  859: 'elective',\n",
       "  860: 'traumatic',\n",
       "  861: 'abortion',\n",
       "  862: 'finally',\n",
       "  863: 'patel',\n",
       "  864: 'which',\n",
       "  865: 'women',\n",
       "  866: 'anoint',\n",
       "  867: 'saint',\n",
       "  868: 'like',\n",
       "  869: 'status',\n",
       "  870: 'johns',\n",
       "  871: 'show',\n",
       "  872: 'that',\n",
       "  873: 'women',\n",
       "  874: 'endorsement',\n",
       "  875: 'through',\n",
       "  876: 'their',\n",
       "  877: 'adherence',\n",
       "  878: 'traditional',\n",
       "  879: 'gender',\n",
       "  880: 'roles',\n",
       "  881: 'associate',\n",
       "  882: 'egalitarian',\n",
       "  883: 'gender',\n",
       "  884: 'roles',\n",
       "  885: 'positively',\n",
       "  886: 'associate',\n",
       "  887: 'with',\n",
       "  888: 'with',\n",
       "  889: 'motherhood',\n",
       "  890: 'indeed',\n",
       "  891: 'women',\n",
       "  892: 'reproductive',\n",
       "  893: 'port',\n",
       "  894: 'women',\n",
       "  895: 'autonomy',\n",
       "  896: 'abortion',\n",
       "  897: 'debate',\n",
       "  898: 'these',\n",
       "  899: 'describe',\n",
       "  900: 'with',\n",
       "  901: 'more',\n",
       "  902: 'warm',\n",
       "  903: 'positive',\n",
       "  904: 'attribute',\n",
       "  905: 'find',\n",
       "  906: 'highlight',\n",
       "  907: 'unique',\n",
       "  908: 'negative',\n",
       "  909: 'association',\n",
       "  910: 'between',\n",
       "  911: 'than',\n",
       "  912: 'their',\n",
       "  913: 'counterparts',\n",
       "  914: 'past',\n",
       "  915: 'reproduction',\n",
       "  916: 'traditional',\n",
       "  917: 'gender',\n",
       "  918: 'role',\n",
       "  919: 'attitudes',\n",
       "  920: 'support',\n",
       "  921: 'legalize',\n",
       "  922: 'chrisler',\n",
       "  923: 'chrisler',\n",
       "  924: 'also',\n",
       "  925: 'find',\n",
       "  926: 'that',\n",
       "  927: 'pregnant',\n",
       "  928: 'abortion',\n",
       "  929: 'women',\n",
       "  930: 'mother',\n",
       "  931: 'young',\n",
       "  932: 'infants',\n",
       "  933: 'more',\n",
       "  934: 'personality',\n",
       "  935: 'social',\n",
       "  936: 'psychology',\n",
       "  937: 'bulletin',\n",
       "  938: 'more',\n",
       "  939: 'likely',\n",
       "  940: 'complete',\n",
       "  941: 'study',\n",
       "  942: 'well',\n",
       "  943: 'identify',\n",
       "  944: 'plausible',\n",
       "  945: 'mechanism',\n",
       "  946: 'respect',\n",
       "  947: 'their',\n",
       "  948: 'counterparts',\n",
       "  949: 'have',\n",
       "  950: 'hysterec',\n",
       "  951: 'behind',\n",
       "  952: 'their',\n",
       "  953: 'association',\n",
       "  954: 'study',\n",
       "  955: 'fill',\n",
       "  956: 'tomies',\n",
       "  957: 'post',\n",
       "  958: 'menopausal',\n",
       "  959: 'critically',\n",
       "  960: 'posi',\n",
       "  961: 'important',\n",
       "  962: 'literature',\n",
       "  963: 'specifically',\n",
       "  964: 'past',\n",
       "  965: 'research',\n",
       "  966: 'tively',\n",
       "  967: 'associate',\n",
       "  968: 'with',\n",
       "  969: 'favorable',\n",
       "  970: 'evaluations',\n",
       "  971: 'both',\n",
       "  972: 'women',\n",
       "  973: 'have',\n",
       "  974: 'solely',\n",
       "  975: 'assess',\n",
       "  976: 'association',\n",
       "  977: 'between',\n",
       "  978: 'ambivalent',\n",
       "  979: 'with',\n",
       "  980: 'young',\n",
       "  981: 'infants',\n",
       "  982: 'pregnant',\n",
       "  983: 'women',\n",
       "  984: 'abortion',\n",
       "  985: 'attitudes',\n",
       "  986: 'cross',\n",
       "  987: 'sectional',\n",
       "  988: 'data',\n",
       "  989: 'another',\n",
       "  990: 'which',\n",
       "  991: 'women',\n",
       "  992: 'pressure',\n",
       "  993: 'conform',\n",
       "  994: 'huang',\n",
       "  995: 'osborne',\n",
       "  996: 'davies',\n",
       "  997: 'such',\n",
       "  998: 'traditional',\n",
       "  999: 'gender',\n",
       "  ...}}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'dict_data/word2vec' # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old user data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = pd.read_csv('https://inroads-test-bucket1.s3.amazonaws.com/Taxonomy-Grid+view+(1).csv', dtype={'text': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                RecordID       Term      Domain      Subdomain\n",
       "0     rec8EgTumvHTEJdLa   abstract  Conceptual     Conceptual\n",
       "1     recMJFlgfo00roAuw    absence  Conceptual     Conceptual\n",
       "2     recAuJIeX88OP6ZIb  abundance  Conceptual     Conceptual\n",
       "3     recz0CPCI09cP5iUX    reality  Conceptual     Conceptual\n",
       "4     rec8Bm0aJwpSKdOGQ  beautiful  Conceptual     Conceptual\n",
       "...                 ...        ...         ...            ...\n",
       "1016  recMKWuPq8dMo70HD     summer    Temporal  Units of time\n",
       "1017  recjrhx9mX0O1xCu0     autumn    Temporal  Units of time\n",
       "1018  recraEJIJ7Fe5acU5       1971    Temporal  Units of time\n",
       "1019  recvyH9ENCss8n1xG       1972    Temporal  Units of time\n",
       "1020  recKH4SX6UXD9wvKv        NaN         NaN            NaN\n",
       "\n",
       "[1021 rows x 4 columns]>"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = old_data[['Term','Domain']].copy().sort_values(['Term','Domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data.to_csv('inroads_old_data.csv')\n",
    "old_data.to_pickle('inroads_old.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list()\n",
    "\n",
    "for i in range(0,len(old_data)):\n",
    "    temp = str(old_data.Term.iloc[i])     \n",
    "    temp = text_lowercase(temp) #make all lower\n",
    "    temp_2 = remove_numbers(temp) # remove all numbers\n",
    "    temp_3 = str.strip(temp_2) # remove white spaces 1\n",
    "    temp_4 = temp_3.replace('\\n',' ').replace(\"\\t\", \" \") # remove white spaces 2\n",
    "    temp_5 = re.sub(\" +\", \" \", temp_4) # remove white spaces 3\n",
    "    temp_6 = remove_punctuation(temp_5) # remove punctuation \n",
    "    temp_7 = lemmatize_word(temp_6) # lemmatize text\n",
    "    temp_8 = [word for word in temp_7 if word.isalnum()] # remove auxiliarly punctuation\n",
    "    temp_9 = remove_short_words(temp_8) # remove short words\n",
    "    result.append(list(filter(None, temp_9))) # remove empty strings and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data['clean_text'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "724              []\n",
       "1018             []\n",
       "1019             []\n",
       "928     [september]\n",
       "682              []\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_data.clean_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tokens = []\n",
    "\n",
    "for i in range(0,len(old_data)):\n",
    "    text = old_data.clean_text[i]\n",
    "    #print(text)\n",
    "    old_tokens.extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          tokens\n",
       "0      abstract\n",
       "1       absence\n",
       "2     abundance\n",
       "3       reality\n",
       "4     beautiful\n",
       "...         ...\n",
       "1307     winter\n",
       "1308     spring\n",
       "1309     season\n",
       "1310     summer\n",
       "1311     autumn\n",
       "\n",
       "[1312 rows x 1 columns]>"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_tokens = pd.DataFrame(old_tokens,columns=['tokens'])\n",
    "old_tokens.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tokens.to_pickle('old_tokens.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.8.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (1.14.27)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Requirement already satisfied: boto in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.17.27)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.4.5.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.27->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.27->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.cluster import KMeans;\n",
    "#from sklearn.neighbors import KDTree\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = list(final_inroads_clean.clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can use the same model I use, which is saved, I provide code to download it from s3.  Or, you can create your own by uncommenting the training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_features = 300\n",
    "#min_word_count = 50\n",
    "#num_workers = 2\n",
    "#window_size = 6\n",
    "#subsampling = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Word2Vec(\n",
    "#    token_list, \n",
    "#    workers = num_workers, \n",
    "#    size = num_features, \n",
    "#    min_count=min_word_count, \n",
    "#    window=window_size, \n",
    "#    sample=subsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need both pieces to make the model work, I'm struggling to find a way for me to upload and download the gensim model from S3.  TO BE DETERMINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'S3' object has no attribute 'get_bucket'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-8a52ac1b6282>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_gensim_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-195-8a52ac1b6282>\u001b[0m in \u001b[0;36mdownload_gensim_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     s3_conn = boto3.client('s3', aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'),\n\u001b[1;32m      8\u001b[0m                     aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'))\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS3_BUCKET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS3_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    573\u001b[0m         raise AttributeError(\n\u001b[1;32m    574\u001b[0m             \"'%s' object has no attribute '%s'\" % (\n\u001b[0;32m--> 575\u001b[0;31m                 self.__class__.__name__, item)\n\u001b[0m\u001b[1;32m    576\u001b[0m         )\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'S3' object has no attribute 'get_bucket'"
     ]
    }
   ],
   "source": [
    "import boto\n",
    "from boto.s3.connection import OrdinaryCallingFormat\n",
    "\n",
    "S3_BUCKET = 'inroads-test-bucket1'\n",
    "S3_KEY = 'w2v_model.bin'\n",
    "\n",
    "def download_gensim_model():\n",
    "    s3_conn = boto3.client('s3', aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "                    aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'))\n",
    "    bucket = s3_conn.get_bucket(S3_BUCKET)\n",
    "    key = bucket.get_key(S3_KEY)\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(key, binary=True)\n",
    "    return model\n",
    "\n",
    "model = download_gensim_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"inroads_word2vec_model\"\n",
    "model = Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abortions', 0.5601829290390015),\n",
       " ('postabortion', 0.4012739658355713),\n",
       " ('contraception', 0.3701522946357727),\n",
       " ('abor', 0.36626535654067993),\n",
       " ('medically', 0.34925901889801025),\n",
       " ('antenatal', 0.3442828059196472),\n",
       " ('termination', 0.3441293239593506),\n",
       " ('reproductive', 0.3407558798789978),\n",
       " ('misoprostol', 0.338686466217041),\n",
       " ('contraceptive', 0.33505553007125854)]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"abortion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(text):\n",
    "    vector_list = []\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        word = text[i]     \n",
    "        try: \n",
    "            vector = model.wv[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "        else:\n",
    "            if vector is None:\n",
    "                vector = np.zeros(300,dtype=int) \n",
    "                vector_list.append([word, vector])\n",
    "            else: \n",
    "                vector_list.append([word, vector])\n",
    "        #vector_embed.append(pd.Series([word, vector], index=['token',range(0,300)]), ignore_index=True)\n",
    "                    \n",
    "    return vector_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "982"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list_old = list(old_tokens.tokens)\n",
    "token_list_old = list(dict.fromkeys(token_list_old))\n",
    "len(token_list_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_vectors = get_vector(token_list_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_vocab = list()\n",
    "\n",
    "for i in range(0,len(old_vectors)):\n",
    "    test = old_vectors[i]\n",
    "    old_vocab.append(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_vocab_df = pd.DataFrame(old_vocab, columns=[\"token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_vectors_2 = list()\n",
    "\n",
    "for i in range(0,len(old_vectors)):\n",
    "    test = old_vectors[i]\n",
    "    old_vectors_2.append(test[1])\n",
    "    \n",
    "old_array = np.array(old_vectors_2)\n",
    "old_vectors_df = pd.DataFrame(old_vectors, columns=[\"token\", \"vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          token                                             vector         0  \\\n",
       "0     abstract  [0.009340104, -0.011540192, -0.005010838, 0.17...  0.009340   \n",
       "1      absence  [0.07312082, -0.047603212, 0.091147356, 0.1621...  0.073121   \n",
       "2    abundance  [0.013253822, 0.05022224, -0.08944513, 0.08348...  0.013254   \n",
       "3      reality  [0.017231788, -0.13826157, -0.06955002, 0.1615...  0.017232   \n",
       "4    beautiful  [-0.056970224, 0.02030195, -0.024677064, -0.04... -0.056970   \n",
       "..         ...                                                ...       ...   \n",
       "936     winter  [-0.021195991, -0.011310648, -0.07302275, 0.03... -0.021196   \n",
       "937     spring  [-0.028904684, 0.093232386, -0.058348857, -0.0... -0.028905   \n",
       "938     season  [-0.074280344, 0.093354344, -0.14504637, 0.006... -0.074280   \n",
       "939     summer  [-0.04597636, -0.006390778, -0.056196004, -0.0... -0.045976   \n",
       "940     autumn  [0.014605279, 0.1677676, -0.08277614, -0.01768...  0.014605   \n",
       "\n",
       "            1         2         3         4         5         6         7  \\\n",
       "0   -0.011540 -0.005011  0.170543 -0.070042 -0.022673  0.035479 -0.033519   \n",
       "1   -0.047603  0.091147  0.162190 -0.052149 -0.108441 -0.028311 -0.041613   \n",
       "2    0.050222 -0.089445  0.083483 -0.044747 -0.065899  0.019941 -0.052136   \n",
       "3   -0.138262 -0.069550  0.161501 -0.024385 -0.121781 -0.035268  0.007963   \n",
       "4    0.020302 -0.024677 -0.048865  0.064803 -0.033035 -0.029016 -0.020143   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "936 -0.011311 -0.073023  0.030679  0.117935  0.040947  0.024909  0.073402   \n",
       "937  0.093232 -0.058349 -0.013379 -0.086675  0.104365  0.064961  0.024115   \n",
       "938  0.093354 -0.145046  0.006174  0.000712  0.073791  0.088202  0.058804   \n",
       "939 -0.006391 -0.056196 -0.057425 -0.018857 -0.025275  0.079501  0.044921   \n",
       "940  0.167768 -0.082776 -0.017683 -0.045675  0.065280  0.051255 -0.045690   \n",
       "\n",
       "     ...       290       291       292       293       294       295  \\\n",
       "0    ...  0.027858  0.024611 -0.013685  0.042488  0.054621 -0.036383   \n",
       "1    ...  0.018856  0.007211 -0.080637  0.157568 -0.035902 -0.001056   \n",
       "2    ...  0.043838  0.002926 -0.050873 -0.012946 -0.007152  0.148603   \n",
       "3    ...  0.027821  0.020316 -0.028523  0.028286 -0.049850 -0.049433   \n",
       "4    ... -0.071911 -0.002086  0.023876  0.077053  0.024279 -0.043584   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "936  ... -0.079502  0.096267  0.060664  0.034208 -0.119383  0.066091   \n",
       "937  ...  0.079233 -0.081211 -0.045705  0.038298 -0.008489 -0.004637   \n",
       "938  ... -0.028003 -0.029557 -0.022236 -0.088852 -0.003609  0.031037   \n",
       "939  ...  0.089390  0.021395  0.031801  0.014382 -0.027417  0.028251   \n",
       "940  ... -0.044892 -0.049401  0.061406  0.076096  0.013065  0.041824   \n",
       "\n",
       "          296       297       298       299  \n",
       "0    0.018394 -0.045638  0.025203 -0.052491  \n",
       "1   -0.014947 -0.114950 -0.121481  0.031619  \n",
       "2    0.109223 -0.019303  0.029999  0.042774  \n",
       "3    0.133848  0.008867 -0.076711  0.042498  \n",
       "4   -0.053606  0.007344  0.018627 -0.046889  \n",
       "..        ...       ...       ...       ...  \n",
       "936 -0.133915  0.001732 -0.072120  0.064670  \n",
       "937 -0.072350  0.039764 -0.014958  0.071050  \n",
       "938  0.097972  0.119025 -0.085960 -0.042356  \n",
       "939 -0.089081 -0.034247 -0.061705 -0.033233  \n",
       "940 -0.068288 -0.040960 -0.040674  0.035157  \n",
       "\n",
       "[941 rows x 302 columns]>"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_vectors_df_2 = pd.DataFrame(old_vectors_df.vector.tolist())\n",
    "old_vectors_df_2.head\n",
    "\n",
    "result = pd.concat([old_vectors_df, old_vectors_df_2], axis=1)\n",
    "result.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_on_wordvecs(word_vectors, num_clusters):\n",
    "    # Initalize a k-means object and use it to extract centroids\n",
    "    kmeans_clustering = KMeans(n_clusters = num_clusters, init='k-means++');\n",
    "    idx = kmeans_clustering.fit_predict(word_vectors);\n",
    "    \n",
    "    return kmeans_clustering.cluster_centers_, idx;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(index2word, k, centers, wordvecs):\n",
    "    tree = KDTree(wordvecs);\n",
    "#Closest points for each Cluster center is used to query the closest 20 points to it.\n",
    "    closest_points = [tree.query(np.reshape(x, (1, -1)), k=k) for x in centers];\n",
    "    closest_words_idxs = [x[1] for x in closest_points];\n",
    "#Word Index is queried for each position in the above array, and added to a Dictionary.\n",
    "    closest_words = {};\n",
    "    for i in range(0, len(closest_words_idxs)):\n",
    "        closest_words['Cluster #' + str(i)] = [index2word[j] for j in closest_words_idxs[i][0]]\n",
    "#A DataFrame is generated from the dictionary.\n",
    "    df = pd.DataFrame(closest_words);\n",
    "    df.index = df.index+1\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers,clusters = clustering_on_wordvecs(old_vectors_2, 9);\n",
    "centroid_map = dict(zip(old_vocab, clusters));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abstract</td>\n",
       "      <td>0         absence\n",
       "1       abundance\n",
       "2         ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0      0\n",
       "1      6\n",
       "2      0\n",
       "3      4\n",
       "4      0\n",
       " ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      token                                            cluster\n",
       "0  abstract  0         absence\n",
       "1       abundance\n",
       "2         ...\n",
       "1         0  0      0\n",
       "1      6\n",
       "2      0\n",
       "3      4\n",
       "4      0\n",
       " ..."
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_map_df = pd.DataFrame(list(centroid_map.items()),columns = ['token','cluster']) \n",
    "centroid_map_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import saved cluster structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = np.load(\"cluster_9_centers.npy\")\n",
    "clusters = np.load(\"cluster_9_clusters.npy\")\n",
    "centroid_map = pd.read_csv(\"cluster_9.csv\", header=None)\n",
    "centroid_map.columns = [\"token\", \"cluster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abstract</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>absence</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abundance</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reality</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>winter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>spring</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>season</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>summer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>autumn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>941 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         token  cluster\n",
       "0     abstract        0\n",
       "1      absence        0\n",
       "2    abundance        6\n",
       "3      reality        0\n",
       "4    beautiful        4\n",
       "..         ...      ...\n",
       "936     winter        1\n",
       "937     spring        1\n",
       "938     season        1\n",
       "939     summer        1\n",
       "940     autumn        1\n",
       "\n",
       "[941 rows x 2 columns]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Cluster #0 Cluster #1       Cluster #2 Cluster #3   Cluster #4  \\\n",
      "1            value       legs         violence       june         wife   \n",
      "2            moral       feet           sexual   december       cousin   \n",
      "3       individual     yellow           stigma    october      husband   \n",
      "4           ethics      bleed             rape      april         aunt   \n",
      "5        knowledge   sidewalk   discrimination    morning     daughter   \n",
      "6          concept      sleep  criminalization       july      brother   \n",
      "7           social    stomach            women     august       father   \n",
      "8         morality    abdomen       stigmatize       week  grandmother   \n",
      "9          process     toilet            abuse     summer    boyfriend   \n",
      "10        autonomy    garbage   stigmatization   february       sister   \n",
      "11           norms     purple             harm    january         girl   \n",
      "12           human      stick         children      night        uncle   \n",
      "13            need    scissor      promiscuity  afternoon  grandfather   \n",
      "14         purpose    bicycle        unmarried   thursday       friend   \n",
      "15        religion       bike        injustice   saturday   girlfriend   \n",
      "16       essential       coat      infanticide      march      funeral   \n",
      "17       religious    bullets          poverty  september        niece   \n",
      "18            form     breath         unwanted   november       auntie   \n",
      "19  responsibility       cord            crime      month         baby   \n",
      "20        identity       boat    incarceration       days       nephew   \n",
      "\n",
      "       Cluster #5    Cluster #6      Cluster #7     Cluster #8  \n",
      "1      government          feel    christianity    misoprostol  \n",
      "2      opposition          love      literature   mifepristone  \n",
      "3           right          good       caribbean       surgical  \n",
      "4          policy        sorrow          europe      pregnancy  \n",
      "5       political          hope          period  complications  \n",
      "6           power         grief           islam     dilatation  \n",
      "7           state       sadness        buddhism        medical  \n",
      "8     legislation          fear           world       referral  \n",
      "9        equality       comfort        cultural     ultrasound  \n",
      "10       movement          pity            asia      antenatal  \n",
      "11      coalition          pain          africa  contraceptive  \n",
      "12        support    compassion     catholicism        uterine  \n",
      "13       congress          what         judaism       cervical  \n",
      "14        liberal  satisfaction        feminism       delivery  \n",
      "15         action          wish           latin      trimester  \n",
      "16       campaign         happy       narrative   postabortion  \n",
      "17    progressive        person            book           larc  \n",
      "18       national        regret  fundamentalism       clinical  \n",
      "19  organizations     gratitude        feminist      gestation  \n",
      "20         system      pleasure          poetry         uterus  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "top_words = get_top_words(old_vocab, 20, centers, old_array)\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge vectors with clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "token       object\n",
       "vector      object\n",
       "0          float64\n",
       "1          float64\n",
       "2          float64\n",
       "            ...   \n",
       "296        float64\n",
       "297        float64\n",
       "298        float64\n",
       "299        float64\n",
       "cluster     object\n",
       "Length: 303, dtype: object"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.merge(result, centroid_map, how='left', on=['token'])\n",
    "training_data = pd.merge(old_vocab_df, centroid_map, how='left', on=['token'])\n",
    "training_data.to_csv('/home/ec2-user/SageMaker/deploy/training_data.csv',index=False)\n",
    "\n",
    "result['cluster'] = result.cluster.astype(str)\n",
    "result.dtypes \n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "bucket = 'sagemaker-word2vec-scikitlearn'\n",
    "region = 'us-east-2'\n",
    "s3_session = boto3.Session().resource('s3')\n",
    "s3_session.Bucket(bucket).Object('train/train.csv').upload_file('cluster_9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0253bc4b06ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://sagemaker-word2vec-scikitlearn.s3.us-east-2.amazonaws.com/train/train.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     )\n\u001b[1;32m    433\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 642\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "data_location = 'https://sagemaker-word2vec-scikitlearn.s3.us-east-2.amazonaws.com/train/train.csv'\n",
    "\n",
    "train_data = pd.read_csv(data_location, header=None, names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.dropna()\n",
    "y = result.cluster\n",
    "X = result.iloc[:, 2:302]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6779661016949152"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "clf.predict(X_test)\n",
    "\n",
    "clf.predict_proba(X_test)\n",
    "\n",
    "clf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)\n",
    "\n",
    "lin_clf = svm.LinearSVC()\n",
    "clf = lin_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.94915254237289"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)*100\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given the two relative accuracies it appears that logistic regression is just as powerful as svm, thus we can continue with logistic regression for ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd sagemaker_inroads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
